# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pVPXIvI9BJrbCCP9iykV4OvDgq10K190
    
This file contains several helper funtion for pre-processing
"""

import numpy as np
import librosa
import matplotlib.pyplot as plt
import glob

# produce emotion label

# 0 - calm; 1 - happy; 2 - sad; 3 - angry; 4 - surprised
target_map = {'02':0,'03':1,'04':2,'05':3,'08':4}


def target_generation(file_name):
    '''
    Generates the target of that audio file. Target will be one of the 5 emotions
    
    Parameter:
        file_name:str
            file_path of audio file
    Returns:
        target: int
            index that represents the target
    '''
    labels = file_name.split('.')[0].split('-')
    emotion = labels[0]

    if emotion not in target_map:
        return None
    
    return target_map[emotion]



# mel_decomposition
def mel_spectral_decomposition(sample,sampling_rate, title = ' title placeholder', visualise = False):
    '''
    Computes the mel-scale log-spectrogram.        
    
    Parameters
    ----------
    sample : numpy.array
        audio samples
    sampling_rate : int
        sampling rate of the audio
    title : str, optional
        title of the spectrogram plot. Ignore if visualise is False. The default is ' title placeholder'.
    visualise : boolean, optional
        boolean value to generate plot. The default is False.

    Returns
    -------
    spectrogram : numpy.ndarray
        array representation of spectrogram.Shape: (freq,times)

    '''

    spectrogram = librosa.feature.melspectrogram(y=sample, sr=sampling_rate, n_mels=128,fmax=8000) 
    spectrogram = librosa.power_to_db(spectrogram)
    
    if visualise:
      librosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time', sr =sampling_rate);
      plt.title(title)
      plt.colorbar(format='%+2.0f dB')
    
    return spectrogram

def truncate_silence(sample):
    '''
    Truncates silence timesteps before and after the speech 
    

    Parameters
    ----------
    sample : numpy.array
        audio samples

    Returns
    -------
    truncated_sample: numpy.array
        truncated sample

    '''
    speech = np.where(abs(sample) > 1e-3)
    
    start = speech[0][0]
    end = speech[0][-1]
    
    return sample[start:end+1]

def amp_normalisation(sample):
    '''
    Normalise the amplitude of audio file.
    i.e [x - mean(x)]/std(x)

    Parameters
    ----------
    sample : numpy.array
        audio sample

    Returns
    -------
    normalised_sample: numpy.array
        normalised_sample

    '''
    mean = np.mean(sample)
    std = np.std(sample)
    return (sample - mean)/std

def pre_pad(samples, max_sample):
    '''
    pad the samples to the max number of samples

    Parameters
    ----------
    samples : numpy.array
        audio samples
    max_sample : int
        max number of samples

    Returns
    -------
    padded_sample : numpy.array
        padded audio sample

    '''
    
    sample_duration = samples.shape[0]
    
    num_to_pad = max_sample - sample_duration
    padded_sample = np.pad(samples,(num_to_pad,0),'constant', constant_values = 0)
    
    return padded_sample



def data_gen(sample, sampling_rate ,duration):
    '''
    This function generates the data for deep learning uses.
    It pre-processes each raw audio sample by:
        1. truncate silence
        2. normalise waveform
        3. pre-pad or crop to duration
        4. compute  mel-spectrogram

    Parameters
    ----------
    sample : numpy.array
        audio samples
    sampling_rate : int
        sampling rate of audio
    duration : int
        duration to keep or pre-pad to

    Returns
    -------
    spectrogram : numpy.ndarray
        mel-spectrogram of samples

    '''
    
    truncated_sample = truncate_silence(sample)
    truncated_sample = amp_normalisation(truncated_sample)
    
    total_duration = truncated_sample.shape[0]
    diff_duration = total_duration - (duration * sampling_rate)
    
    padded_sample = truncated_sample
    if diff_duration < 0:
        padded_sample = pre_pad(truncated_sample, int(duration * sampling_rate))
      
    spectrogram = mel_spectral_decomposition(padded_sample[:int(sampling_rate * duration)], sampling_rate)

    return spectrogram



def load_samples(model_folder,sampling_rate = 16000,
                 padding = True, 
                 truncating = True, 
                 normal = True,
                 statement_type = [1,2] ,duration = 2):
    '''
    Every sample in the dataset and pre-process it. 
    NOTE: this is only applicable for clean dataset at the moment

    Parameters
    ----------
    model_folder : str
        path to the project folder. e.g. .../STAT3007_Project
    sr : int, optional
        sampling_rate. The default is 16000.
    padding : boolean, optional
        Whether to apply padding. The default is True.
    truncating : boolean, optional
        Wether to apply truncation. The default is True.
    normal : boolean, optional
        Wether to apply amplitude normalisation. The default is True.
    statement_type : list, optional
        statement type to include. 1 - dog, 2 - kids. The default is [1,2].
    duration : int, optional
        maximum duration of the audio to include. The default is 2.

    Returns
    -------
    X : list
        complete set of samples in a list. Each element is an array with the shape (freq,duration)
    y : list
        complete set of targets corresponding to the samples. Its values maps the 5 emotions to 0-4

    '''
    
    # load samples
    X = []
    y = []
    
    for folder_name in glob.glob(model_folder + '/Audio_Speech_Actors_01-24/*'):
        for actor_folder in glob.glob(folder_name + '/*'):
            for sample_path in glob.glob(actor_folder + '/*.wav'):
              
              sample_name = sample_path.split('/')[-1]
              
              emotion, intensity, repetition, statement, actor = tuple(sample_name.split('-')[:5])
        
              
              # skip unwanted emotions and normal intensity
              if emotion not in target_map or intensity == '01':
                  continue
            
              # skip unwanted statements
              if statement == '01' and 1 not in statement_type:
                  continue
              
              if statement == '02' and 2 not in statement_type:
                  continue
        
              sample, sampling_rate = librosa.load(sample_path, sr = sampling_rate)
              
              # truncate the silence of the audio sample
              truncated_sample = sample
        
              if truncating:
                  truncated_sample = truncate_silence(sample)
              
              
              # check the difference between the maximum duration and the sample duration
              total_duration = truncated_sample.shape[0]
              diff_duration = total_duration - (duration * sampling_rate)
        
              #normalisation
              if normal:
                  truncated_sample = amp_normalisation(truncated_sample)
              
        
              #pre pading the sample with zeros if it is shorter than the maximum duration
              # else, include only the first 2 seconds of it
              padded_sample = truncated_sample
        
              if padding and diff_duration < 0:
                  padded_sample = pre_pad(truncated_sample, int(duration * sampling_rate))
                
        
              spectrogram = mel_spectral_decomposition(padded_sample[:int(sampling_rate * duration)], sampling_rate)
        
              target = target_generation(sample_name)
        
              X.append(spectrogram)
              y.append(target)
    
    return X, y