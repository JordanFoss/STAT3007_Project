# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aJ0Kgl8Tq4qA71ZKwxxdTxBok9GLYrQ2
"""

# Commented out IPython magic to ensure Python compatibility.
#!git clone https://github.com/JordanFoss/STAT3007_Project.git
# %cd STAT3007_Project/



import numpy as np
import librosa
from librosa.effects import split
import matplotlib.pyplot as plt
import glob
from torch.utils.data import DataLoader, Dataset, random_split
import torch
from sklearn.model_selection import train_test_split

from pre_process import *

class DatasetWrapper(Dataset):
    '''
    A Wrapper for data to form a dataset 
    '''
    def __init__(self, X, y):
        self.X, self.y = X, y
      
    def __len__(self):
        return len(self.X)
      
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
    
    def change_type(self, dtype):
      
        return DatasetWrapper(self.X.type(dtype),self.y.type(dtype))
    
    def dataset(self):
        return DatasetWrapper(self.X,self.y)
    
    def get_data(self):
        return self.X, self.y
      
def load_samples(model_folder,sampling_rate = 16000,
                 padding = True, 
                 truncating = True, 
                 normal = True,
                 statement_type = [1,2] ,duration = 2,
                 Noisfy = False):
    '''
    Every sample in the dataset and pre-process it. 
    NOTE: this is only applicable for clean dataset at the moment

    Parameters
    ----------
    model_folder : str
        path to the project folder. e.g. .../STAT3007_Project
    sr : int, optional
        sampling_rate. The default is 16000.
    padding : boolean, optional
        Whether to apply padding. The default is True.
    truncating : boolean, optional
        Wether to apply truncation. The default is True.
    normal : boolean, optional
        Wether to apply amplitude normalisation. The default is True.
    statement_type : list, optional
        statement type to include. 1 - dog, 2 - kids. The default is [1,2].
    duration : int, optional
        maximum duration of the audio to include. The default is 2.

    Returns
    -------
    X : list
        complete set of samples in a list. Each element is an array with the shape (freq,duration)
    y : list
        complete set of targets corresponding to the samples. Its values maps the 5 emotions to 0-4

    '''
    
    # load samples
    X = []
    y = []
    
    for folder_name in glob.glob(model_folder + '/Audio_Speech_Actors_01-24/*'):
        for actor_folder in glob.glob(folder_name + '/*'):
            for sample_path in glob.glob(actor_folder + '/*.wav'):
              
              sample_name = sample_path.split('/')[-1]
              
              emotion, intensity, repetition, statement, actor = tuple(sample_name.split('-')[:5])
        
              
              # skip unwanted emotions and normal intensity
              if emotion not in target_map or intensity == '01':
                  continue
            
              # skip unwanted statements
              if statement == '01' and 1 not in statement_type:
                  continue
              
              if statement == '02' and 2 not in statement_type:
                  continue
        
              sample, sampling_rate = librosa.load(sample_path, sr = sampling_rate)
              
              # truncate the silence of the audio sample
              truncated_sample = sample
        
              if truncating:
                  truncated_sample = truncate_silence(sample)
              
              
              # check the difference between the maximum duration and the sample duration
              total_duration = truncated_sample.shape[0]
              diff_duration = total_duration - (duration * sampling_rate)
        
              #normalisation
              if normal:
                  truncated_sample = amp_normalisation(truncated_sample)
              
        
              #pre pading the sample with zeros if it is shorter than the maximum duration
              # else, include only the first 2 seconds of it
              padded_sample = truncated_sample
        
              if padding and diff_duration < 0:
                  padded_sample = pre_pad(truncated_sample, int(duration * sampling_rate))
                
              if Noisfy:
                  thing = np.random.random_sample()
                  if thing < 0.2:
                      color = NoiseColour.White
                  elif thing < 0.4:
                        color = NoiseColour.Brown
                  elif thing < 0.6:
                        color = NoiseColour.Violet
                  elif thing < 0.8:
                        color = NoiseColour.Blue
                  else:
                        color = NoiseColour.Pink
                  padded_sample = torch.tensor(padded_sample)
                  padded_sample = nosify(padded_sample, colour = color)
                  padded_sample = padded_sample.detach().numpy()
              spectrogram = mel_spectral_decomposition(padded_sample[:int(sampling_rate * duration)], sampling_rate)
        
              target = target_generation(sample_name)
        
              X.append(spectrogram)
              y.append(target)
    
    return X, y

def load_noisy_samples(model_folder):
    # load samples
    X = []
    y = []
    
    for gender_folder in glob.glob(model_folder + '/*'):
        for actor_folder in glob.glob(gender_folder + '/*'):
            for sample_path in glob.glob(actor_folder + '/*.wav'):
              
              sample_name = sample_path.split('/')[-1]
              
              emotion, intensity, repetition, statement, actor = tuple(sample_name.split('-')[:5])

              sample, sampling_rate = librosa.load(sample_path, sr = 16000)
              mel_spectrogram = data_gen(sample, sampling_rate ,2)
              
              target = target_generation(sample_name)
        
              X.append(mel_spectrogram)
              y.append(target)
    
    return X, y

def load_sets(X,y,train_ratio = [0.7,0.7], seed = [10,11]):
    '''
    Loading training and test sets. It has the option to split up to twice

    Parameters
    ----------
    X : list
        list of mel-spectrograms
    y : list
        emotion labels
    train_ratio : list, optional
        train_ratios of first and second split. The default is [0.7,0.7].
    seed : list, optional
        random seeds for first and second split. The default is [10,11].

    Returns
    -------
    data_sets : list
        list of train_test split subsets

    '''
    data_sets = []
    for i in range(len(train_ratio)):
        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = train_ratio[i], random_state = seed[i])
        
        data_train = DatasetWrapper(X_train, y_train)
        data_test = DatasetWrapper(X_test, y_test)
        
        data_sets.append((data_train,data_test))
        X,y = X_test,y_test
        
    return data_sets


def load_train_and_test_spectrograms(noisy_base_dir, clean_base_dir, 
                                     actors_in_train= ['01','02','03','04',
                                                       '05','06','07','08',
                                                       '09','10','11','12',
                                                       '13','14','15','16'],
                                     actors_in_test= ['17','18','19','20',
                                                      '21','22','23','24']):
    '''
    Generate array of noisy training spectrograms, 
             array of noisy testing spectrograms, 
             array of clean training spectrograms,
             array of clean testing spectrograms.
             array of training targets
             array of testing targets
             
    Parameters
    ----------
    noisy_base_dir : str
        directory containing noisy audios
        
    clean_base_dir : str
        directory containing clean audios
    
    actors_in_train: actors to put in training set
    actors_in_test: actors to put in testing set
    '''
    

    noisy_train = []
    noisy_test = []
    clean_train = []
    clean_test = []
    target_train = []
    target_test = []

    for gender_folder in glob.glob(noisy_base_dir + '/*'):
        noisy_dir = os.path.join(noisy_base_dir, gender_folder.split('/')[-1])
        clean_dir = os.path.join(clean_base_dir, gender_folder.split('/')[-1])
        for actor_folder in glob.glob(gender_folder + '/*'):
            print(os.path.basename(actor_folder).split('_')[-1])
            if (os.path.basename(actor_folder).split('_')[-1] not in actors_in_train and 
                os.path.basename(actor_folder).split('_')[-1] not in actors_in_test):
                continue

            noisy_dir2 = os.path.join(noisy_dir, actor_folder.split('/')[-1])
            clean_dir2 = os.path.join(clean_dir, actor_folder.split('/')[-1])
            for noisy_sample_path in glob.glob(actor_folder + '/*.wav'):
                sample_base = noisy_sample_path.split('/')[-1]
                sample_name = sample_base.split('_')[0]
                
                # generate target
                target = target_generation(sample_name)
                # noisy spectrogram
                noisy_sample, noisy_sampling_rate = librosa.load(noisy_sample_path, sr = 16000)
                noisy_mel_spectrogram = data_gen(noisy_sample, noisy_sampling_rate ,2)
                # clean spectrogram
                clean_file = os.path.join(clean_dir2, sample_name + '.wav')
                clean_sample, clean_sampling_rate = librosa.load(clean_file, sr=16000)
                clean_mel_spectrogram = data_gen(clean_sample, clean_sampling_rate, 2)
                # put in train or test array depends on the actor
                actor = sample_name.split('-')[-1]
                # print(actor)
                if actor in actors_in_train:
                    noisy_train.append(noisy_mel_spectrogram)
                    clean_train.append(clean_mel_spectrogram)
                    target_train.append(target)
                else:
                    noisy_test.append(noisy_mel_spectrogram)
                    clean_test.append(clean_mel_spectrogram)
                    target_test.append(target)
            print(os.path.basename(actor_folder) + ' done')
    
    return np.asarray(noisy_train), np.asarray(noisy_test), np.asarray(clean_train),np.asarray(clean_test), np.asarray(target_train), np.asarray(target_test)
        
def load_new_spectrograms(noisy_base_dir, clean_base_dir):
    '''
    Generating noisy spectrograms for audio data in noisy directory.
    Generate corresponding clean spectrograms by searching in the clean directory.

    Parameters
    ----------
    noisy_base_dir : str
        directory containing noisy audios
        
    clean_base_dir : str
        directory containing clean audios
    
    Returns
    -------
    noisy_spectrograms : numpy array of noisy spectrograms
    clean_spectrograms : numpy array of clean spectrograms   
    targets : numpy array of targets of each piece of noisy/clean data

    '''
    
    noisy_spectrogram = []
    clean_spectrogram = []
    targets = []
    count = 0
    for noisy_sample_path in glob.glob(noisy_base_dir + '/*.wav'):
        basename = os.path.basename(noisy_sample_path).split('_')[0]
        series = basename.split('-')
        if int(series[-1]) % 2 == 0:
            clean_dir = os.path.join(clean_base_dir, 'Female')
        else:
            clean_dir = os.path.join(clean_base_dir, 'Male')
        clean_file = os.path.join(clean_dir, 'Actor_'+series[-1])
        clean_file = os.path.join(clean_file, basename+'.wav')

        # generate target
        target = target_generation(basename)
        # noisy spectrogram
        noisy_sample, noisy_sampling_rate = librosa.load(noisy_sample_path, sr = 16000)
        noisy_mel_spectrogram = data_gen(noisy_sample, noisy_sampling_rate ,2)
        # clean spectrogram
        clean_sample, clean_sampling_rate = librosa.load(clean_file, sr=16000)
        clean_mel_spectrogram = data_gen(clean_sample, clean_sampling_rate, 2)

        noisy_spectrogram.append(noisy_mel_spectrogram)
        clean_spectrogram.append(clean_mel_spectrogram)
        targets.append(target)

        count += 1
        print(count)
    
    return np.asarray(noisy_spectrogram), np.asarray(clean_spectrogram), np.asarray(targets)

def dataset_train_test_split(noisy_spectrograms, clean_spectrograms, targets, train_ratio=[0.6, 0.8], seed=[10,11]):
    '''
    Perform the train test split on noisy spectrograms, clean spectrograms and targets
    We typically perform the split twice, the first time on the whole dataset, the second
    time is performed on the test set of the first time.

    Parameters
    ----------
    noisy_spectrograms : numpy array
        array containing all noisy spectrograms
        
    clean_base_dir : numpy array
        directory containing clean audios
    
    targets : numpy array
        array of targets 
    
    train_ratio: list
        A list whose length indicates the number of split to do (we only need 2)
        the number indicates the training ratio each time
        
    seed: list
        A list containing the random state of each split
    Returns
    -------

    '''
    # The length is determined by the number of splits
    noisy_train_list = []
    clean_train_list = []
    target_train_list = []
    
    noisy_test_list = []
    clean_test_list = []
    target_test_list = []
    
    for i in range(len(train_ratio)):
        noisy_train, noisy_test, clean_train, clean_test, target_train, target_test = train_test_split(
        noisy_spectrograms, clean_spectrograms, targets, train_size=train_ratio[i], random_state=seed[i])
        
        noisy_train_list.append(noisy_train)
        clean_train_list.append(clean_train)
        target_train_list.append(target_train)
        
        noisy_test_list.append(noisy_test)
        clean_test_list.append(clean_test)
        target_test_list.append(target_test)
        
        # next split will be done on the test data
        noisy_spectrograms = noisy_test
        clean_spectrograms = clean_test
        targets = target_test
    
    return noisy_train_list, clean_train_list, target_train_list, noisy_test_list, clean_test_list, target_test_list
